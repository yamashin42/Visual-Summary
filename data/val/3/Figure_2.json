{"caption": "Figure 6. Examples of attention patterns in SciBERT model for (a) higher-ranked sample and (b) lower ranked sample.",
 "mention": "To understand the model behavior, we analyze the attention patterns in Transformer models. We then visualize the attention maps from SciBERT to compare the samples that are ranked higher and lower by the model. From the visualization, we observe that most attention patterns are similar to those reported by Kovaleva et al. (2019), including vertical blocks, and heterogeneous blocks. As shown in Figure 6, we find some heads in SciBERT focusing on the lexical overlap between abstract and caption. In the example of the higher ranked sample (Figure 6 (a)), some tokens like ’mask’ and ’strategies’ are used both in abstract and caption and have mutually high attention weights. Additionally, tokens ’different’ and ’various’ 369 are used in similar meanings and show high attention weights. In contrast, in the lower ranked sample (Figure 6), abstract and caption do not share semantically similar tokens except preposition ’with’ and therefore we cannot find the interaction between two sentences. This observation suggests that the lexical overlap between abstract and caption could be a basis for model’s judgement."
}