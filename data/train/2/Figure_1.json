{"caption": "Figure 1. Illustration of our proposed self-supervised learning for central figure identification. (a) Existing supervised learning approach (Yang et al., 2019) requires labeled data for model training. (b) Our proposed self-supervised learning utilizes an inline reference to figure, which can be obtained without manual effort.",
    "mention":" Based on our workshop paper (Yamamoto et al., 2021), we tackle the problems mentioned above by (1) building a novel benchmark for central figure identification, which consists of computer science papers from several subdomains, and (2) proposing a self-supervised learning approach without the use of manually annotated data for model training. To build our proposed benchmark for the task, we hire two (semi-expert) annotators to read a paper’s abstract and rank the top three figures to be the best candidates for the GA. We collect papers presented at several conferences of four subdomains, concretely, natural language processing (NLP), computer vision (CV), artificial intelligence (AI), and machine learning (ML). The novel data set enables us to evaluate the performance of models for central figure identification across different (sub)domains. Consequently, we can evaluate the robustness of a method of central figure identification to domain difference. Secondly, we introduce a self-supervised learning approach for central figure identification to remove the need for manual annotation for model training. In our approach, instead of employing a ground-truth label indicating the central figure, we utilize an inline reference to the figure (Figure 1): in the body of the article, a figure is usually mentioned with a direct reference like “In Figure 3, we illustrate · · · ”, which typically indicates that the content of the mentioning paragraph (with the inline reference) is relevant to what the mentioned figure illustrates. We create pairs of paragraphs in an article’s body and mentioned figures as training data. To evaluate the effectiveness of proposed self-supervised learning, we then build and train a standard Transformer-based (Vaswani et al., 2017) model to predict a score that reflects whether a given paragraph from the article is connected to a caption of a paired figure. At inference time, we consider pairs of abstracts and figure captions as the model’s input to predict whether the content of the figure matches the article’s abstract (i.e., the overview of the article). This stands in contrast to sentence matching (Bowman et al., 2015; Wang et al., 2017; Liu et al., 2019a; Duan et al., 2018), which is usually treated as a sentence-pair classification task, as we cast the problem as a ranking task according to how much the content of the figures match to the article’s abstract. Without the use of manually labeled data for training, our self-supervised approach outperforms the existing fully supervised learning approach (Yang et al., 2019) in terms of top-1 accuracy on the existing data set consisting of PubMed publications. Finally, we provide a comparison of central figure identification across training data from different domains."}